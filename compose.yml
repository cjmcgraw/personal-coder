services:

  ollama:
    build: ./ollama
    volumes:
      - ./.models:/ollama-models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  webui:
    build: ./webui
    volumes:
      - webui_data:/app/backend/data
    ports:
      - 8999:8080
    environment:
      OLLAMA_BASE_URL: http://rag-proxy:3000

  rag-proxy:
    build: ./rag-proxy
    volumes:
      - ./rag-proxy:/app
    environment:
      - OLLAMA_URL=http://ollama:11434
      - RAG_URL=http://rag:8000
      - CODE_EXPERT_MODEL=code-expert:latest,gemma3:4b  # Models that should use RAG
    ports:
      - "3000:3000"  # Optional - exposed for debugging
    depends_on:
      - ollama
      - rag

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    ports:
      - 9200:9200
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - path.repo=/usr/share/elasticsearch/data
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 10s
      timeout: 5s
      retries: 3

  rag:
    build: ./rag
    volumes:
      - external_code:/code:ro  # Mount your codebase as read-only
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      - ELASTICSEARCH_INDEX=code-embeddings
      - AUTO_INGEST_ON_STARTUP=true
      - CODE_DIR=/code
    ports:
      - "8000:8000"
    depends_on:
      elasticsearch:
        condition: service_healthy
      ollama:
        condition: service_started

volumes:
  external_code:
    driver: local
    driver_opts:
      type: none
      o: ro
      device: ${RAG_CODE_BASE:?err}
  elasticsearch_data:
  webui_data:
